{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["7dLwFdlWDQmR","MD1xb12DuGuW","zXB0BmMYt6rq","x1Ms8amFj3fE","EJV0htafevfm","Qpup75tyezA9","ISV2I_fia-oS"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bH-xEOkcp-y9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f12ff687-1931-494f-dc59-5348ac3e5283","executionInfo":{"status":"ok","timestamp":1674839618194,"user_tz":-60,"elapsed":22306,"user":{"displayName":"Michele Carlo La Greca","userId":"08422419523344327391"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Packages"],"metadata":{"id":"7dLwFdlWDQmR"}},{"cell_type":"markdown","source":["## Download packages"],"metadata":{"id":"MD1xb12DuGuW"}},{"cell_type":"code","source":["!pip install split_folders"],"metadata":{"id":"6ByCbXPXEiGE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674839641633,"user_tz":-60,"elapsed":5579,"user":{"displayName":"Michele Carlo La Greca","userId":"08422419523344327391"}},"outputId":"2fdfd004-e994-4a2f-d572-823ac00521b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting split_folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split_folders\n","Successfully installed split_folders-0.5.1\n"]}]},{"cell_type":"markdown","source":["## Importing packages"],"metadata":{"id":"zXB0BmMYt6rq"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import shutil \n","import random\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import confusion_matrix\n","from PIL import Image\n","from zipfile import ZipFile, error\n","from tqdm import tqdm\n","from pathlib import Path\n","import splitfolders\n","import math\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers"],"metadata":{"id":"DRoXAjcRuGG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"],"metadata":{"id":"sSx1soaDuKvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","import logging\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","tf.get_logger().setLevel('INFO')\n","tf.autograph.set_verbosity(0)\n","\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.get_logger().setLevel('ERROR')\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"],"metadata":{"id":"ESL6-_i9uMeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Splitting"],"metadata":{"id":"x1Ms8amFj3fE"}},{"cell_type":"code","source":["initial_zip_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/train_set.zip'\n","initial_folder_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data'\n","csv_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data/train/labels_train.csv'\n","csv2_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data/labels_train.csv'\n","train_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data/train'\n","project_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project'"],"metadata":{"id":"2f7g9VlRuFaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Splitting by images"],"metadata":{"id":"EJV0htafevfm"}},{"cell_type":"code","source":["data_splitted_path = r'/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data/data_splitted'"],"metadata":{"id":"oe2uhcWPxQQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splitfolders.ratio(\n","    train_path, \n","    output=data_splitted_path, \n","    seed=1337, # or seed already prepared before\n","    ratio=(.7, .15, .15), \n","    group_prefix=None, \n","    move=False)\n","# it takes more time for the first images of each class folder\n"],"metadata":{"id":"pSe3l12ij4p-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We count the number of images in train, val and test."],"metadata":{"id":"6nm8o7nhy0_A"}},{"cell_type":"code","source":["split = [\"train\", \"val\", \"test\"]\n","for s in split: \n","  print(s, \" has:\")\n","  main_path = os.path.join(data_splitted_path, s)\n","  c = 0 \n","  p = 0\n","  for i in [\"N\",\"P\",\"T\"]: \n","    dir_path = os.path.join(main_path, i)\n","    count = 0\n","    path_size = 0\n","    # Iterate directory\n","    for path in os.listdir(dir_path):\n","        # check if current path is a file\n","        if os.path.isfile(os.path.join(dir_path, path)):\n","          count += 1\n","          path_size += os.path.getsize(os.path.join(dir_path, path))\n","    print(\"   Class \", i, \" has \", count, \" images\", \"and the total size is:\", path_size*1e-9, \"GB\")\n","    c = c + count\n","    p = p + path_size\n","  print(s, \" has \", c, \" images and\", p*1e-9, \"GB of size\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaBHcq-Bvcdv","outputId":"244556f3-fd65-4c50-a355-4b77fa086558","executionInfo":{"status":"ok","timestamp":1671008239410,"user_tz":-60,"elapsed":113057,"user":{"displayName":"Michele Carlo La Greca","userId":"08422419523344327391"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train  has:\n","   Class  N  has  6547  images and the total size is: 1.742229352 GB\n","   Class  P  has  2975  images and the total size is: 0.835284938 GB\n","   Class  T  has  1306  images and the total size is: 0.706987785 GB\n","train  has  10828  images and 3.2845020750000002 GB of size\n","\n","val  has:\n","   Class  N  has  1403  images and the total size is: 0.331561949 GB\n","   Class  P  has  637  images and the total size is: 0.169287922 GB\n","   Class  T  has  279  images and the total size is: 0.16079147200000002 GB\n","val  has  2319  images and 0.661641343 GB of size\n","\n","test  has:\n","   Class  N  has  1404  images and the total size is: 0.361314511 GB\n","   Class  P  has  638  images and the total size is: 0.17924994300000002 GB\n","   Class  T  has  281  images and the total size is: 0.14848792700000002 GB\n","test  has  2323  images and 0.689052381 GB of size\n","\n"]}]},{"cell_type":"code","source":["def get_size(start_path = project_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(start_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            # skip if it is symbolic link\n","            if not os.path.islink(fp):\n","                total_size += os.path.getsize(fp)\n","\n","    return total_size\n","\n","print(get_size(), 'bytes')"],"metadata":{"id":"h79Wh4534eyC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Splitting by patient"],"metadata":{"id":"Qpup75tyezA9"}},{"cell_type":"markdown","source":["### Full split"],"metadata":{"id":"ISV2I_fia-oS"}},{"cell_type":"markdown","source":["We create the output directories."],"metadata":{"id":"wtdf5jzvo98D"}},{"cell_type":"code","source":["os.makedirs('/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data_splitted_patient')\n","os.makedirs('/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data_splitted_patient/train')\n","os.makedirs('/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data_splitted_patient/val')\n","os.makedirs('/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data_splitted_patient/test')\n","base_outdir = '/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data_splitted_patient/'"],"metadata":{"id":"dcWO1286bBkg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We split the data."],"metadata":{"id":"cqAcBX1zpB1B"}},{"cell_type":"code","source":["root_dir = '/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/train'\n","classes = ['N', 'P', 'T']\n","folders = ['train', 'val', 'test']\n","\n","for clss in classes:\n","  dirtry = root_dir + '/' + clss\n","  files = os.listdir(dirtry)\n","  np.random.shuffle(files)\n","  for folder in folders:\n","    target_dir = base_outdir + folder\n","    if not os.path.exists(target_dir + '/' + clss):\n","      os.makedirs(target_dir + '/' + clss)\n","    target_class = target_dir + '/' + clss\n","\n","    if folder == 'train':\n","      images_to_pass = files[: math.floor(0.6*len(files))]\n","      for img in images_to_pass:\n","        #check if the patient of this image is not in the other splits\n","\n","        folders_two = [x for x in folders if x != folder] # i take the other two splits\n","        copied = False\n","        for fold in folders_two:\n","          if copied == True: break\n","          path1 = base_outdir + '/' + fold\n","          for c in os.listdir(path1):\n","            path2 = path1 + '/' + c\n","            files2 = os.listdir(path2) # all the images copied so far in this folder\n","            result = [i for i in files2 if i.startswith(img[:7])]\n","            if len(result) != 0:\n","              # i have the same patient in this folder then i must copy the image in this split\n","              new_target = base_outdir + '/' + fold + '/' + clss\n","              img_dir = dirtry + '/' + img\n","              shutil.copy(img_dir, new_target)\n","              copied = True\n","              break\n","\n","        if copied == False: # we didnt find any same patient in other split\n","          img_dir = dirtry + '/' + img\n","          shutil.copy(img_dir, target_class)\n","          #print(img + 'dont have any prefix in ' + fold + folder)\n","    \n","    elif folder == 'val':\n","      images_to_pass = files[math.floor(0.6*len(files)): math.floor(0.8*len(files))]\n","      for img in images_to_pass:\n","        #check if the patient of this image is not in the other splits\n","\n","        folders_two = [x for x in folders if x != folder] # i take the other two splits\n","        copied = False\n","        for fold in folders_two:\n","          if copied == True: break\n","          path1 = base_outdir + '/' + fold\n","          for c in os.listdir(path1):\n","            path2 = path1 + '/' + c\n","            files2 = os.listdir(path2) # all the images copied so far in this folder\n","            result = [i for i in files2 if i.startswith(img[:7])]\n","            if len(result) != 0:\n","              # i have the same patient in this folder then i must copy the image in this split\n","              new_target = base_outdir + '/' + fold + '/' + clss\n","              img_dir = dirtry + '/' + img\n","              shutil.copy(img_dir, new_target)\n","              copied = True\n","              break\n","\n","        if copied == False: # we didnt find any same patient in other split\n","          img_dir = dirtry + '/' + img\n","          shutil.copy(img_dir, target_class)\n","          #print(img + 'dont have any prefix in ' + fold + folder)\n","    else:\n","      images_to_pass = files[math.floor(0.8*len(files)):]\n","      for img in images_to_pass:\n","        #check if the patient of this image is not in the other splits\n","\n","        folders_two = [x for x in folders if x != folder] # i take the other two splits\n","        copied = False\n","        for fold in folders_two:\n","          if copied == True: break\n","          path1 = base_outdir + '/' + fold\n","          for c in os.listdir(path1):\n","            path2 = path1 + '/' + c\n","            files2 = os.listdir(path2) # all the images copied so far in this folder\n","            result = [i for i in files2 if i.startswith(img[:7])]\n","            if len(result) != 0:\n","              # i have the same patient in this folder then i must copy the image in this split\n","              new_target = base_outdir + '/' + fold + '/' + clss\n","              img_dir = dirtry + '/' + img\n","              shutil.copy(img_dir, new_target)\n","              copied = True\n","              break\n","\n","        if copied == False: # we didnt find any same patient in other split\n","          img_dir = dirtry + '/' + img\n","          shutil.copy(img_dir, target_class)"],"metadata":{"id":"7pxKjQv0pBef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We check the number of images."],"metadata":{"id":"lGdOjo_0pZHi"}},{"cell_type":"code","source":["split = [\"train\", \"val\", \"test\"]\n","for s in split: \n","  print(s, \" has:\")\n","  main_path = os.path.join('/content/drive/MyDrive/Courses/Applied AI for Biomedicine/Project/data_splitted_patient', s)\n","  c = 0 \n","  p = 0\n","  for i in [\"N\",\"P\",\"T\"]: \n","    dir_path = os.path.join(main_path, i)\n","    count = 0\n","    path_size = 0\n","    # Iterate directory\n","    for path in os.listdir(dir_path):\n","        # check if current path is a file\n","        if os.path.isfile(os.path.join(dir_path, path)):\n","          count += 1\n","          path_size += os.path.getsize(os.path.join(dir_path, path))\n","    print(\"   Class \", i, \" has \", count, \" images\", \"and the total size is:\", path_size*1e-9, \"GB\")\n","    c = c + count\n","    p = p + path_size\n","  print(s, \" has \", c, \" images and\", p*1e-9, \"GB of size\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tEhxVi8xpYTm","executionInfo":{"status":"ok","timestamp":1671886311628,"user_tz":-60,"elapsed":118487,"user":{"displayName":"Michele Carlo La Greca","userId":"08422419523344327391"}},"outputId":"2152287f-06f7-4d4f-ae82-e59e7b353cc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train  has:\n","   Class  N  has  6647  images and the total size is: 1.7770441620000001 GB\n","   Class  P  has  3025  images and the total size is: 0.874228153 GB\n","   Class  T  has  1320  images and the total size is: 0.7481700910000001 GB\n","train  has  10992  images and 3.3994424060000004 GB of size\n","\n","val  has:\n","   Class  N  has  1506  images and the total size is: 0.37489813200000005 GB\n","   Class  P  has  678  images and the total size is: 0.174640596 GB\n","   Class  T  has  303  images and the total size is: 0.14397784700000002 GB\n","val  has  2487  images and 0.6935165750000001 GB of size\n","\n","test  has:\n","   Class  N  has  1201  images and the total size is: 0.283163518 GB\n","   Class  P  has  547  images and the total size is: 0.13495405400000002 GB\n","   Class  T  has  243  images and the total size is: 0.124119246 GB\n","test  has  1991  images and 0.542236818 GB of size\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Tj_X5ARgZCkm"},"execution_count":null,"outputs":[]}]}